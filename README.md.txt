
#### üïπÔ∏èBaseline

##### LLM 
| Title                                                        |        Model        |  Date   |                             Code                             | Venue |
| :----------------------------------------------------------- | :-----------------: | :-----: | :----------------------------------------------------------: | :---: |
| [**Synthetic medical education in dermatology leveraging generative artificial intelligence**]|   GPT-4   | 2025 |      none       | npj Digital Medicine |
| [**Large language models encode clinical knowledge**] |   Med-PaLM  | 08/2023 |      none      | Nature |
| [**A multimodal vision foundation model for clinical dermatology**] |   PanDerm   | 04/2024 |      [project page]()       | CVPR |
| [**An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering Using a VLM**](https://arxiv.org/abs/2403.18406) |   IG-VLM   | 03/2024 |      [code](https://github.com/imagegridworth/IG-VLM)       | arXiv |
| [**Language repository for long video understanding**](https://arxiv.org/abs/2403.14622) |   LangRepo   | 03/2024 |      [code](https://github.com/kkahatapitiya/LangRepo)       | arXiv |
| [**Understanding long videos in one multimodal language model pass**](https://arxiv.org/abs/2403.16998) |   MVU   | 03/2024 |      [code](https://github.com/kahnchana/mvu)       | arXiv |
| [**Video ReCap recursive captioning of hour-long videos**](https://arxiv.org/abs/2402.13250) |   Video ReCap   | 02/2024 |      [code](https://sites.google.com/view/vidrecap)       | CVPR |
| [**A Simple LLM Framework for Long-Range Video Question-Answering**](https://arxiv.org/abs/2312.17235) |   LLoVi   | 12/2023 |      [code](https://github.com/CeeZh/LLoVi)       | arXiv |
| [**Grounding-prompter prompting LLM with multimodal information for temporal sentence grounding in long videos**](https://arxiv.org/abs/2312.17117) |   Grounding-prompter   | 12/2023 |      [code]()       | arXiv |
| [**Learning object state changes in videos an open-world perspective**](https://arxiv.org/abs/2312.11782) |   VIDOSC   | 12/2023 |      [code]()       | CVPR |
| [**AntGPT: Can Large Language Models Help Long-term Action Anticipation from Videos?**](https://arxiv.org/abs/2307.16368) |   AntGPT   | 07/2023 |      [code](https://brown-palm.github.io/AntGPT)       | ICLR |
| [**VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset**](https://arxiv.org/abs/2305.18500v1)[![Star](https://img.shields.io/github/stars/txh-mercury/vast?style=social&label=Star)](https://github.com/txh-mercury/vast) |  VAST   | 05/2023 |    [code](https://github.com/txh-mercury/vast)     | NeurIPS |
| [**VLog: Video as a Long Document**](https://github.com/showlab/VLog)[![Star](https://img.shields.io/github/stars/showlab/VLog.svg?style=social&label=Star)](https://github.com/showlab/VLog) |        VLog         | 04/2023 |    [code](https://huggingface.co/spaces/TencentARC/VLog)     |   -   |
| [**Learning Video Representations from Large Language Models**](https://arxiv.org/abs/2212.04501)[![Star](https://img.shields.io/github/stars/facebookresearch/lavila?style=social&label=Star)](https://github.com/facebookresearch/lavila) | LaViLa  | 12/2022 | [code](https://github.com/facebookresearch/lavila) |  CVPR   |

#####Medical LLM
| Title                                                        |        Model        |  Date   |                             Code                             | Venue |
| :----------------------------------------------------------- | :-----------------: | :-----: | :----------------------------------------------------------: | :---: |
| [**HuatuoGPT-o1: Towards Medical Complex Reasoning with LLMs**]|  HuatuoGPT-o1  | 2024 |      [code](https://github.com/FreedomIntelligence/HuatuoGPT-o1)       | arXiv |
| [**DermETAS-SNA LLM: A Dermatology Focused Evolutionary Transformer Architecture Search with StackNet Augmented LLM Assistant**] |   DermETAS-SNA LLM Assistant   | 2025 |     none     | CNS |
| [**MedReason: Eliciting Factual Medical Reasoning Steps in LLMs via Knowledge Graphs**] |   MedReason-8B   | 04/2025 |      [code](https://github.com/UCSC-VLAA/MedReason)       | arXiv |
| [**Quantifying the reasoning abilities of LLMs on clinical cases**] |  none   |11/2025 |      [code](https://github.com/MAGIC-AI4Med/MedRBench)      | Nature Communications|
| [**EPIQAL: Benchmarking Large Language Models in Epidemiological Question Answering for Enhanced Alignment and Reasoning**] |   EpiQAL   | 01/2026 |      [code](https://github.com/myweiii/EpiQAL)       | arXiv |
| [**MedRedFlag: Investigating how LLMs Redirect Misconceptions in Real-World Health Communication**]|   none   | 2026 |     none       | arXiv |
| [**MIMIC-RD: Can LLMs Differentially Diagnose Rare Diseases in Real-World Clinical Settings?**] |   none   | 2025 |      [code](https://github.com/zelal-Eizaldeen/rare_disease_pyHealth/tree/main)       | ML4H |
| [**MLB: A Scenario-Driven Benchmark for Evaluating Large Language Models in Clinical Applications**] |   none   | 2026 |    none      | arXiv |
| [**ReGraM: Region-First Knowledge Graph Reasoning for Medical Question Answering**] |   ReGraM   | 2026 |     none      | arXiv |
| [**SELF-MEDRAG: A Self-Reflective Hybrid Retrieval-Augmented Generation Framework for Reliable Medical Question Answering**]| Self-MedRAG  | 2026 |none| Journal of Engineering Science and Technology |
| [**Implications of Integrating Large Language Models into Clinical Decision Making**] |   none   | 2026 |      none      | Communications Medicine |
| [**VURF a general-purpose reasoning and self-refinement framework for video understanding**](https://arxiv.org/abs/2403.14743) |   VURF   | 03/2024 |      [code]()       | arXiv |
| [**Why not use your textbook knowledge-enhanced procedure planning of instructional videos**](https://arxiv.org/abs/2403.02782) |   KEPP   | 03/2024 |      [code]()       | CVPR |
| [**DoraemonGPT toward understanding dynamic scenes with large language models**](https://arxiv.org/abs/2401.08392) |   DoraemonGPT   | 01/2024 |      [code](https://github.com/z-x-yang/DoraemonGPT)       | arXiv |
| [**LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos**](https://arxiv.org/abs/2312.05269) |   LifelongMemory   | 12/2023 |      [code](https://github.com/Agentic-Learning-AI-Lab/lifelong-memory)       | arXiv |
| [**Zero-Shot Video Question Answering with Procedural Programs**](https://arxiv.org/abs/2312.00937) |   ProViQ   | 12/2023 |      [code](https://rccchoudhury.github.io/proviq2023)       | arXiv |
| [**AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn**](https://arxiv.org/abs/2306.08640) |   AssistGPT   | 06/2023 |      [code](https://showlab.github.io/assistgpt/)       | arXiv |
| [**ChatVideo: A Tracklet-centric Multimodal and Versatile Video Understanding System**](https://arxiv.org/abs/2304.14407) |      ChatVideo      | 04/2023 |    [project page](https://www.wangjunke.info/ChatVideo/)     | arXiv |
| [**Video ChatCaptioner: Towards Enriched Spatiotemporal Descriptions**](https://arxiv.org/abs/2304.04227)[![Star](https://img.shields.io/github/stars/Vision-CAIR/ChatCaptioner.svg?style=social&label=Star)](https://github.com/Vision-CAIR/ChatCaptioner/tree/main/Video_ChatCaptioner) | Video ChatCaptioner | 04/2023 | [code](https://github.com/Vision-CAIR/ChatCaptioner/tree/main/Video_ChatCaptioner) | arXiv |
| [**ViperGPT: Visual Inference via Python Execution for Reasoning**](https://arxiv.org/abs/2303.08128) |   ViperGPT   | 03/2023 |      [code](https://viper.cs.columbia.edu/)       | arXiv |
| [**Hawk: Learning to Understand Open-World Video Anomalies**](https://arxiv.org/abs/2405.16886) |   Hawk   | 05/2024 |      [code](https://github.com/jqtangust/hawk)       | arXiv |


##### Agent

| Title                                                        |        Model        |  Date   |                             Code                             | Venue |
| :----------------------------------------------------------- | :-----------------: | :-----: | :----------------------------------------------------------: | :---: |
| [**TxAgent: An AI Agent for Therapeutic Reasoning Across a Universe of Tools**]|   TxAgent   | 2025 |      [code](https://github.com/mims-harvard/TxAgent )     | CNS |
| [**ADASEARCH: Adaptive Search with Explicit Self-Knowledge Awareness**] |   ADASEARCH   | 2025 |    [code](https://github.com/hank0316/AdaSearch)      | arXiv |
| [**FUAS-AGENTS: AUTONOMOUS MULTI-MODAL LLM AGENTS FOR TREATMENT PLANNING IN FOCUSED ULTRASOUND ABLATION SURGERY**] |   FUAS-Agents   |2026|      [code](https://anonymous.4open.science/r/FUAS-7D56)       | ICLR |
| [**Fewer tokens and fewer videos extending video understanding abilities in large vision-language models**](https://arxiv.org/abs/2406.08024) |   FTFV-LLM   | 06/2024 |      -      | arXiv |
| [**Flash-VStream: Memory-Based Real-Time Understanding for Long Video Streams**](https://arxiv.org/abs/2406.08085) |   Flash-VStream   | 06/2024 |      [code](https://invinciblewyq.github.io/vstream-page/)       | arXiv |
| [**LLAVIDAL benchmarking large language vision models for daily activities of living**](https://arxiv.org/abs/2406.09390) |   LLAVIDAL   | 06/2024 |      [code](https://adl-x.github.io/)       | arXiv |
| [**Long context transfer from language to vision**](https://arxiv.org/abs/2406.16852) |   LongVA   | 06/2024 |      [code](https://github.com/EvolvingLMMs-Lab/LongVA)       | arXiv |
| [**ShareGPT4Video improving video understanding and generation with better captions**](https://arxiv.org/abs/2406.04325) |   ShareGPT4Video   | 06/2024 |      [code](https://sharegpt4video.github.io/)       | arXiv |
| [**Towards event-oriented long video understanding**](https://arxiv.org/abs/2406.14129) |   VIM   | 06/2024 |      [code](https://github.com/RUCAIBox/Event-Bench)       | arXiv |
| [**Video-SALMONN speech-enhanced audio-visual large language models**](https://arxiv.org/abs/2406.15704) |   Video-SALMONN   | 06/2024 |      [code](https://github.com/bytedance/SALMONN/)       | ICML |
| [**VideoGPT+ integrating image and video encoders for enhanced video understanding**](https://arxiv.org/abs/2406.09418) |   VideoGPT+   | 06/2024 |      [code](https://github.com/mbzuai-oryx/VideoGPT-plus)       | arXiv |
| [**VideoLLaMA 2 advancing spatial-temporal modeling and audio understanding in video-LLMs**](https://arxiv.org/abs/2406.07476) |   VideoLLaMA 2   | 06/2024 |      [code](https://github.com/DAMO-NLP-SG/VideoLLaMA2)       | arXiv |
| [**MotionLLM: Understanding Human Behaviors from Human Motions and Videos**](https://arxiv.org/abs/2405.20340) |   MotionLLM   | 05/2024 |      [project page](https://lhchen.top/MotionLLM)       | arXiv |
| [**MVBench: A Comprehensive Multi-modal Video Understanding Benchmark**](https://arxiv.org/abs/2311.17005) |   VideoChat2   | 11/2023 |      [code](https://github.com/OpenGVLab/Ask-Anything)       | CVPR |
| [**Shotluck Holmes: A Family of Efficient Small-Scale Large Language Vision Models For Video Captioning and Summarization**](https://arxiv.org/abs/2405.20648) |   Shotluck Holmes   | 05/2024 |      -      | arXiv |
| [**Streaming long video understanding with large language models**](https://www.arxiv.org/abs/2405.16009) |   VideoStreaming   | 05/2024 |      -       | arXiv |
| [**Synchronized Video Storytelling: Generating Video Narrations with Structured Storyline**](https://arxiv.org/html/2405.14040v1) |   VideoNarrator   | 05/2024 |      -       | arXiv |
| [**TOPA extend large language models for video understanding via text-only pre-alignment**](https://arxiv.org/abs/2405.13911) |   TOPA   | 05/2024 |      [code](https://github.com/dhg-wei/TOPA)       | NeurIPS |
| [**MovieChat+: Question-aware Sparse Memory for Long Video Question Answering**](https://arxiv.org/abs/2404.17176) |   MovieChat+   | 04/2024 |      [code](https://github.com/rese1f/MovieChat)       | arXiv |
| [**AutoAD III: The Prequel ‚Äì Back to the Pixels**](https://arxiv.org/abs/2404.14412) |   AutoAD III   | 04/2024 |      [project page](https://www.robots.ox.ac.uk/~vgg/research/autoad/)       | CVPR |
| [**Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward**](https://arxiv.org/abs/2404.01258) |   LLaVA-Hound-DPO   | 04/2024 |      [code](https://github.com/RifleZhang/LLaVA-Hound-DPO)       | arXiv |
| [**From image to video, what do we need in multimodal LLMs**](https://arxiv.org/abs/2404.11865) |   RED-VILLM   | 04/2024 |      -       | arXiv |
| [**Koala key frame-conditioned long video-LLM**](https://arxiv.org/abs/2404.04346) |   Koala   | 04/2024 |      [project page](https://cs-people.bu.edu/rxtan/projects/Koala)       | CVPR |
| [**LongVLM efficient long video understanding via large language models**](https://arxiv.org/abs/2404.03384) |   LongVLM   | 04/2024 |      [code](https://github.com/ziplab/LongVLM)       | ECCV |
| [**MA-LMM memory-augmented large multimodal model for long-term video understanding**](https://arxiv.org/abs/2404.05726) |   MA-LMM   | 04/2024 |      [code](https://boheumd.github.io/MA-LMM/)       | CVPR |
| [**MiniGPT4-video advancing multimodal LLMs for video understanding with interleaved visual-textual tokens**](https://arxiv.org/abs/2404.03413) |   MiniGPT4-Video   | 04/2024 |      [code](https://vision-cair.github.io/MiniGPT4-video/)       | arXiv |
| [**Pegasus-v1 technical report**](https://arxiv.org/abs/2404.14687) |   Pegasus-v1   | 04/2024 |      [code]()       | arXiv |
| [**PLLaVA : Parameter-free LLaVA Extension from Images to Videos for Video Dense Captioning**](https://arxiv.org/abs/2404.16994) |   PLLaVA   | 04/2024 |      [code](https://pllava.github.io/)       | arXiv |
| [**ST-LLM: Large Language Models Are Effective Temporal Learners**](https://arxiv.org/abs/2404.00308) |   ST-LLM   | 04/2024 |      [code](https://github.com/TencentARC/ST-LLM)       | arXiv |
| [**Tarsier recipes for training and evaluating large video description models**](https://arxiv.org/abs/2407.00634) |   Tarsier   | 07/2024 |      [code](https://github.com/bytedance/tarsier)       | arXiv |
| [**X-VARS introducing explainability in football refereeing with multi-modal large language model**](https://arxiv.org/abs/2404.06332) |   X-VARS   | 04/2024 |      [code]()       | arXiv |
| [**CAT: Enhancing Multimodal Large Language Model to Answer Questions in Dynamic Audio-Visual Scenarios**](https://arxiv.org/abs/2403.04640) |   CAT   | 03/2024 |      [code](https://github.com/rikeilong/Bay-CAT)       | arXiv |
| [**InternVideo2 scaling video foundation models for multimodal video understanding**](https://arxiv.org/abs/2403.15377) |   InternVideo2   | 03/2024 |      [code](https://github.com/OpenGVLab/InternVideo/tree/main/InternVideo2)       | ECCV |
| [**MovieLLM enhancing long video understanding with AI-generated movies**](https://arxiv.org/abs/2403.01422) |   MovieLLM   | 03/2024 |      [code](https://deaddawn.github.io/MovieLLM/)       | arXiv |
| [**LLMs meet long video advancing long video comprehension with an interactive visual adapter in LLMs**](https://arxiv.org/abs/2402.13546) |   IVAwithLLM   | 02/2024 |      [code]()       | arXiv |
| [**LSTP language-guided spatial-temporal prompt learning for long-form video-text understanding**](https://arxiv.org/abs/2402.16050) |   LSTP   | 02/2024 |      [code](https://github.com/bigai-nlco/VideoTGB)       | EMNLP |
| [**LVCHAT facilitating long video comprehension**](https://arxiv.org/abs/2402.12079) |   LVCHAT   | 02/2024 |      [code](https://github.com/wangyu-ustc/LVChat)       | arXiv |
| [**OSCaR: Object State Captioning and State Change Representation**](https://arxiv.org/abs/2402.17128) |   OSCaR   | 02/2024 |      [code](https://github.com/nguyennm1024/OSCaR)       | NAACL |
| [**Slot-VLM SlowFast slots for video-language modeling**](https://arxiv.org/abs/2402.13088) |   Slot-VLM   | 02/2024 |      [code]()       | arXiv |
| [**COSMO: COntrastive Streamlined MultimOdal Model with Interleaved Pre-Training**](https://arxiv.org/abs/2401.00849) |   COSMO   | 01/2024 |      [code](http://fingerrec.github.io/cosmo)       | arXiv |
| [**Weakly supervised gaussian contrastive grounding with large multimodal models for video question answering**](https://arxiv.org/abs/2401.10711) |   GCG   | 01/2024 |      [code](https://github.com/WHB139426/GCG)       | ACMMM |
| [**Audio-Visual LLM for Video Understanding**](https://arxiv.org/abs/2312.06720) |   AV-LLM   | 12/2023 |      [code]()       | arXiv |
| [**Generative Multimodal Models are In-Context Learners**](https://arxiv.org/abs/2312.13286) |   Emu2   | 12/2023 |      [project page](https://baaivision.github.io/emu2)       | CVPR |
| [**MMICT: Boosting Multi-Modal Fine-Tuning with In-Context Examples**](https://arxiv.org/abs/2312.06363) |   MMICT   | 12/2023 |      [code](https://github.com/KDEGroup/MMICT)       | TOMM |
| [**VaQuitA : Enhancing Alignment in LLM-Assisted Video Understanding**](https://arxiv.org/abs/2312.02310) |   VaQuitA   | 12/2023 |      [code]()       | arXiv |
| [**VILA: On Pre-training for Visual Language Models**](https://arxiv.org/abs/2312.07533) |   VILA   | 12/2023 |      [code](https://github.com/NVlabs/VILA)       | CVPR |
| [**Vista-LLaMA reliable video narrator via equal distance to visual tokens**](https://arxiv.org/abs/2312.08870) |   Vista-LLaMA   | 12/2023 |      [project page](https://jinxxian.github.io/Vista-LLaMA)       | arXiv |
| [**Chat-UniVi unified visual representation empowers large language models with image and video understanding**](https://arxiv.org/abs/2311.08046) |   Chat-UniVi   | 11/2023 |      [code](https://github.com/PKU-YuanGroup/Chat-UniVi)       | CVPR |
| [**LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models**](https://arxiv.org/abs/2311.17043) |   LLaMA-VID   | 11/2023 |      [code](https://github.com/dvlab-research/LLaMA-VID)       | arXiv |
| [**Video-LLaVA learning united visual representation by alignment before projection**](https://arxiv.org/abs/2311.10122) |   Video-LLaVA   | 11/2023 |      [code](https://github.com/PKU-YuanGroup/Video-LLaVA)       | arXiv |
| [**Large Language Models are Temporal and Causal Reasoners for Video Question Answering**](https://arxiv.org/abs/2310.15747) |   LLaMA-VQA   | 10/2023 |      [code](https://github.com/mlvlab/Flipped-VQA)       | EMNLP |
| [**MovieChat: From Dense Token to Sparse Memory for Long Video Understanding**](https://arxiv.org/abs/2307.16449) |   MovieChat   | 07/2023 |      [code](https://rese1f.github.io/MovieChat/)       | CVPR |
| [**LLMVA-GEBC: Large Language Model with Video Adapter for Generic Event Boundary Captioning**](https://arxiv.org/abs/2306.10354) |   LLMVA-GEBC   | 06/2023 |      [code](https://github.com/zjr2000/LLMVA-GEBC)       | CVPR |
| [**Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration**](https://arxiv.org/abs/2306.09093) |   Macaw-LLM   | 06/2023 |      [project page](https://github.com/lyuchenyang/Macaw-LLM)       | arXiv |
| [**Valley: Video Assistant with Large Language model Enhanced abilitY**](https://arxiv.org/abs/2306.07207) |   VALLEY   | 06/2023 |      [code]()       | arXiv |
| [**Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models**](https://arxiv.org/abs/2306.05424) |   Video-ChatGPT   | 06/2023 |      [code](https://github.com/mbzuai-oryx/Video-ChatGPT)       | ACL |
| [**Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding**](https://arxiv.org/abs/2306.02858) |   Video-LLaMA   | 06/2023 |      [code](https://github.com/DAMO-NLP-SG/Video-LLaMA)       | EMNLP |
| [**Youku-mPLUG: A 10 Million Large-scale Chinese Video-Language Dataset for Pre-training and Benchmarks**](https://arxiv.org/abs/2306.04362) |   mPLUG-video   | 06/2023 |      [code](https://github.com/X-PLUG/Youku-mPLUG)       | arXiv |
| [**ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst**](https://arxiv.org/abs/2305.16103) |   ChatBridge   | 05/2023 |      [code](https://iva-chatbridge.github.io)       | arXiv |
| [**Otter: A Multi-Modal Model with In-Context Instruction Tuning**](https://arxiv.org/abs/2305.03726) |   Otter   | 05/2023 |      [code](https://github.com/Luodian/Otter)       | arXiv |
| [**VideoLLM: Modeling Video Sequence with Large Language Models**](https://arxiv.org/abs/2305.13292) |   VideoLLM   | 05/2023 |      [code](https://github.com/cg1177/VideoLLM)       | arXiv |
| [**One Trajectory, One Token: Grounded Video Tokenization via Panoptic Sub-object Trajectory**](https://arxiv.org/abs/2505.23617) |   -   | 05/2025 |      [code](https://github.com/RAIVNLab/trajvit)       | ICCV 2025 |


##### MLLM
<!-- 
| [**title**](link) |   model   | date |      [code](link)       | venue |
 -->
| Title                                                        |        Model        |  Date   |                             Code                             | Venue |
| :----------------------------------------------------------- | :-----------------: | :-----: | :----------------------------------------------------------: | :---: |
|[**LLaVA-MR: Large Language-and-Vision Assistant for Video Moment Retrieval**](https://arxiv.org/pdf/2411.14505)|LLaVA-MR|11/2024|[code]()| arXiv |
| [**Holmes-VAD: Towards Unbiased and Explainable Video Anomaly Detection via Multi-modal LLM**](https://arxiv.org/abs/2406.12235) |   Holmes-VAD   | 06/2024 |      [code](https://holmesvad.github.io/)       | arXiv |
| [**VideoLLM-online online video large language model for streaming video**](https://arxiv.org/abs/2406.11816) |   VideoLLM-online   | 06/2024 |      [code](https://showlab.github.io/videollm-online)       | CVPR |
| [**HOI-Ref: Hand-Object Interaction Referral in Egocentric Vision**](https://arxiv.org/abs/2404.09933) |   VLM4HOI   | 04/2024 |      [project page](https://sid2697.github.io/hoi-ref/)       | arXiv |
| [**V2Xum-LLM cross-modal video summarization with temporal prompt instruction tuning**](https://arxiv.org/abs/2404.12353) |   V2Xum-LLaMA   | 04/2024 |      [code](https://hanghuacs.github.io/v2xum/)       | arXiv |
| [**AVicuna: Audio-Visual LLM with Interleaver and Context-Boundary Alignment for Temporal Referential Dialogue**](https://arxiv.org/abs/2403.16276) |   AVicuna   | 03/2024 |      [code]()       | arXiv |
| [**Elysium exploring object-level perception in videos via MLLM**](https://arxiv.org/abs/2403.16558) |   Elysium   | 03/2024 |      [code](https://github.com/Hon-Wong/Elysium)       | arXiv |
| [**HawkEye training video-text LLMs for grounding text in videos**](https://arxiv.org/abs/2403.10228) |   HawkEye   | 03/2024 |      [code](https://github.com/yellow-binary-tree/HawkEye)       | arXiv |
| [**LITA language instructed temporal-localization assistant**](https://arxiv.org/abs/2403.19046) |   LITA   | 03/2024 |      [code](https://github.com/NVlabs/LITA)       | arXiv |
| [**OmniViD: A Generative Framework for Universal Video Understanding**](https://arxiv.org/abs/2403.17935) |   OmniViD   | 03/2024 |      [code](https://github.com/wangjk666/OmniVid)       | CVPR |
| [**GroundingGPT: Language Enhanced Multi-modal Grounding Model**](https://arxiv.org/abs/2401.06071) |   GroundingGPT   | 01/2024 |      [code](https: //github.com/lzw-lzw/GroundingGPT)       | arXiv |
| [**TimeChat a time-sensitive multimodal large language model for long video understanding**](https://arxiv.org/abs/2312.02051) |   TimeChat   | 12/2023 |      [code](https://github.com/RenShuhuai-Andy/TimeChat)       | CVPR |
| [**Self-Chained Image-Language Model for Video Localization and Question Answering**](https://arxiv.org/abs/2305.06988) |   SeViLA   | 11/2023 |      [code](https://github.com/Yui010206/SeViLA)       | NeurIPS |
| [**VTimeLLM: Empower LLM to Grasp Video Moments**](https://arxiv.org/abs/2311.18445) |   VTimeLLM   | 11/2023 |      [code](https://github.com/huangb23/VTimeLLM)       | arXiv |

#####Medical MLLM
 | Title                                                        |        Model        |  Date   |                             Code                             | Venue |
| :----------------------------------------------------------- | :-----------------: | :-----: | :----------------------------------------------------------: | :---: |
| [**A multimodal vision foundation model for clinical dermatology**] |   PanDerm   | 08/2025 |     none       | Nature Medicine |
| [**Pre-trained multimodal large language model enhances dermatological diagnosis using SkinGPT-4**]|  SkinGPT-4  | 07/2024 |      none      | Nature Communications |
| [**DermoGPT: Open Weights and Open Data for Morphology-Grounded Dermatological Reasoning MLLMs**]|   DermoGPT   | 2026 |      [code](https://github.com/mendicant04/DermoGPT)       | CNS |
| [**MedM2G: Unifying Medical Multi-Modal Generation via Cross-Guided Diffusion with Visual Invariant**] |   MedM2G   | 2024 |      none      | CVPR |
| [**Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning**] |  Lingshu  | 06/2025 |     [code](https://alibaba-damo-academy.github.io/lingshu/ )     | none |
| [**MedGRPO: Multi-Task Reinforcement Learning for Heterogeneous Medical Video Understanding**] |   MedGRPO   | 2025 |     [code]( https://yuhaosu.github.io/MedGRPO/ )   | arXiv |
| [**MEDREASON-R1: LEARNING TO REASON FOR CT DIAGNOSIS WITH REINFORCEMENT LEARNING AND LOCAL ZOOM**] |   MedReason-R1   | 10/2025 |    [code]( https://github.com/Leevan001/MedReason-R1 )      | arXiv |
| [**Aligning Findings with Diagnosis: A Self-Consistent Reinforcement Learning Framework for Trustworthy Radiology Reporting**]|  none  | 01/2026 |     none      | arXiv |
| [**DISCRETE DIFFUSION MODELS WITH MLLMS FOR UNIFIED MEDICAL MULTIMODAL GENERATION**]|  MeDiM  |2026 |     none      | ICLR |
| [**MedTutor-R1: Socratic Personalized Medical Teaching with Multi-Agent Simulation**]| MedTutor-R1 | 12/2025 |     [code](https://github.com/Zhitao-He/MedTutor-R1)   | arXiv |
| [**UniX: Unifying Autoregression and Diffusion for Chest X-Ray Understanding and Generation**]| UniX  | 2026 |     [code](https://github.com/ZrH42/UniX)      | arXiv |
| [**Aligning Findings with Diagnosis: A Self-Consistent Reinforcement Learning Framework for Trustworthy Radiology Reporting**]|  none  | 01/2026 |     none      | arXiv |

##### Networks&Review
<!-- 
| [**title**](link) |   model   | date |      [code](link)       | venue |
 -->
| Title                                                        |        Model        |  Date   |                             Code                             | Venue |
| :----------------------------------------------------------- | :-----------------: | :-----: | :----------------------------------------------------------: | :---: |
|[**Combinatorial prediction of therapeutic perturbations using causally inspired neural networks**]|PDGrapher|07/2025|none| Nature Biomedical Engineering |
| [**Medical Reasoning in the Era of LLMs: A Systematic Review of Enhancement Techniques and Applications**]|   none  | 2025 |      none       | none |
| [**VideoLLM-online online video large language model for streaming video**](https://arxiv.org/abs/2406.11816) |   VideoLLM-online   | 06/2024 |      [code](https://showlab.github.io/videollm-online)       | CVPR |
| [**HOI-Ref: Hand-Object Interaction Referral in Egocentric Vision**](https://arxiv.org/abs/2404.09933) |   VLM4HOI   | 04/2024 |      [project page](https://sid2697.github.io/hoi-ref/)       | arXiv |
| [**V2Xum-LLM cross-modal video summarization with temporal prompt instruction tuning**](https://arxiv.org/abs/2404.12353) |   V2Xum-LLaMA   | 04/2024 |      [code](https://hanghuacs.github.io/v2xum/)       | arXiv |
| [**AVicuna: Audio-Visual LLM with Interleaver and Context-Boundary Alignment for Temporal Referential Dialogue**](https://arxiv.org/abs/2403.16276) |   AVicuna   | 03/2024 |      [code]()       | arXiv |
| [**Elysium exploring object-level perception in videos via MLLM**](https://arxiv.org/abs/2403.16558) |   Elysium   | 03/2024 |      [code](https://github.com/Hon-Wong/Elysium)       | arXiv |
| [**HawkEye training video-text LLMs for grounding text in videos**](https://arxiv.org/abs/2403.10228) |   HawkEye   | 03/2024 |      [code](https://github.com/yellow-binary-tree/HawkEye)       | arXiv |
| [**LITA language instructed temporal-localization assistant**](https://arxiv.org/abs/2403.19046) |   LITA   | 03/2024 |      [code](https://github.com/NVlabs/LITA)       | arXiv |
| [**OmniViD: A Generative Framework for Universal Video Understanding**](https://arxiv.org/abs/2403.17935) |   OmniViD   | 03/2024 |      [code](https://github.com/wangjk666/OmniVid)       | CVPR |
| [**GroundingGPT: Language Enhanced Multi-modal Grounding Model**](https://arxiv.org/abs/2401.06071) |   GroundingGPT   | 01/2024 |      [code](https: //github.com/lzw-lzw/GroundingGPT)       | arXiv |
| [**TimeChat a time-sensitive multimodal large language model for long video understanding**](https://arxiv.org/abs/2312.02051) |   TimeChat   | 12/2023 |      [code](https://github.com/RenShuhuai-Andy/TimeChat)       | CVPR |
| [**Self-Chained Image-Language Model for Video Localization and Question Answering**](https://arxiv.org/abs/2305.06988) |   SeViLA   | 11/2023 |      [code](https://github.com/Yui010206/SeViLA)       | NeurIPS |
| [**VTimeLLM: Empower LLM to Grasp Video Moments**](https://arxiv.org/abs/2311.18445) |   VTimeLLM   | 11/2023 |      [code](https://github.com/huangb23/VTimeLLM)       | arXiv |


